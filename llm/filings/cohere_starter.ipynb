{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cohere\n",
    "import requests\n",
    "from edgar import *\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "from pprint import pprint\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "import hnswlib\n",
    "from typing import List, Dict\n",
    "from unstructured.partition.html import partition_html\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# COHERE_API_KEY = \"c2guXlp1mohZNOKnEpYiFyOtsQNHeU5L99j6JkGF\"\n",
    "\n",
    "co = cohere.Client(COHERE_API_KEY,\n",
    "                   log_warning_experimental_features=False,    \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_ticker_and_range(prompt):\n",
    "    today = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    ticker = co.chat(\n",
    "        model=\"command-r-plus\",\n",
    "        preamble=\"you are going to return only the ticker for the company that the user is asking about. The ticker should be formatted with MAX 4 characters and MIN 2 characters\",\n",
    "        message=prompt,\n",
    "        connectors=[{\"id\": \"web-search\"}]\n",
    "    )\n",
    "    \n",
    "    pprint(ticker.text)\n",
    "    \n",
    "    response = co.chat(\n",
    "    model=\"command-r-plus\",\n",
    "    preamble=f\"I want you to generate a JSON that represents a query that the user made about a company's stock with ticker, formatted with MAX 4 characters and MIN 2 characters, and the start and end date of the query formatted as YYYY-MM-DD. Today is going to be the date {today} if there is no end date known, do the last year.\",\n",
    "    message=f\"{prompt} this information will help you get the ticker: {ticker.text}\", \n",
    "    response_format={\n",
    "            \"type\": \"json_object\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"required\": [\"tickers\", \"start-date\", \"end-date\"],\n",
    "                \"properties\": {\n",
    "                    \"tickers\": { \"type\": \"array\" },\n",
    "                    \"start-date\": { \"type\": \"string\"},\n",
    "                    \"end-date\": { \"type\": \"string\" }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    # connectors= [{\"id\": \"web-search\"}]\n",
    "    )\n",
    "    \n",
    "    response = json.loads(response.text)\n",
    "    \n",
    "    return response[\"tickers\"], response[\"start-date\"], response[\"end-date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'AAPL'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['AAPL'], '2024-09-28', '2023-09-28')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_stock_ticker_and_range(\"What is the stock ticker for Apple?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(prompt):\n",
    "    \n",
    "    ticker, start_date, end_date = get_stock_ticker_and_range(prompt)\n",
    "    loaded_data = yf.download(ticker, start=start_date, end=end_date)\n",
    "\n",
    "    return loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = [\n",
    "    {\n",
    "        \"title\": \"Crafting Effective Prompts\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/crafting-effective-prompts\"},\n",
    "    {\n",
    "        \"title\": \"Advanced Prompt Engineering Techniques\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/advanced-prompt-engineering-techniques\"},\n",
    "    {\n",
    "        \"title\": \"Batch embedding jobs\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/embed-jobs-api\"},\n",
    "    {\n",
    "        \"title\": \"Preambles\",\n",
    "        \"url\": \"https://docs.cohere.com/docs/preambles\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vectorstore:\n",
    "    \"\"\"\n",
    "    A class representing a collection of documents indexed into a vectorstore.\n",
    "\n",
    "    Parameters:\n",
    "    raw_documents (list): A list of dictionaries representing the sources of the raw documents. Each dictionary should have 'title' and 'url' keys.\n",
    "\n",
    "    Attributes:\n",
    "    raw_documents (list): A list of dictionaries representing the raw documents.\n",
    "    docs (list): A list of dictionaries representing the chunked documents, with 'title', 'text', and 'url' keys.\n",
    "    docs_embs (list): A list of the associated embeddings for the document chunks.\n",
    "    docs_len (int): The number of document chunks in the collection.\n",
    "    idx (hnswlib.Index): The index used for document retrieval.\n",
    "\n",
    "    Methods:\n",
    "    load_and_chunk(): Loads the data from the sources and partitions the HTML content into chunks.\n",
    "    embed(): Embeds the document chunks using the Cohere API.\n",
    "    index(): Indexes the document chunks for efficient retrieval.\n",
    "    retrieve(): Retrieves document chunks based on the given query.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, raw_documents: List[Dict[str, str]]):\n",
    "        self.raw_documents = raw_documents\n",
    "        self.docs = []\n",
    "        self.docs_embs = []\n",
    "        self.retrieve_top_k = 10\n",
    "        self.rerank_top_k = 3\n",
    "        self.load_and_chunk()\n",
    "        self.embed()\n",
    "        self.index()\n",
    "\n",
    "\n",
    "    def load_and_chunk(self) -> None:\n",
    "        \"\"\"\n",
    "        Loads the text from the sources and chunks the HTML content in parallel.\n",
    "        \"\"\"\n",
    "        print(\"Loading documents...\")\n",
    "        \n",
    "        def process_document(raw_document):\n",
    "            try:\n",
    "                elements = partition_html(url=raw_document[\"url\"], headers={\n",
    "                    \"User-Agent\": \"ks@gatech.edu\"\n",
    "                })\n",
    "                chunks = chunk_by_title(elements)\n",
    "                return [\n",
    "                    {\n",
    "                        \"title\": raw_document[\"title\"],\n",
    "                        \"text\": str(chunk),\n",
    "                        \"url\": raw_document[\"url\"],\n",
    "                    }\n",
    "                    for chunk in chunks\n",
    "                ]\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading document: {e}\")\n",
    "                return []\n",
    "\n",
    "        # Use ThreadPoolExecutor for parallel processing\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            # Submit all tasks\n",
    "            future_to_doc = {executor.submit(process_document, doc): doc for doc in self.raw_documents}\n",
    "            \n",
    "            # Process results as they complete\n",
    "            for future in tqdm(as_completed(future_to_doc), total=len(self.raw_documents), desc=\"Processing documents\"):\n",
    "                self.docs.extend(future.result())\n",
    "\n",
    "    def embed(self) -> None:\n",
    "        \"\"\"\n",
    "        Embeds the document chunks using the Cohere API.\n",
    "        \"\"\"\n",
    "        print(\"Embedding document chunks...\")\n",
    "\n",
    "        batch_size = 90\n",
    "        self.docs_len = len(self.docs)\n",
    "        for i in range(0, self.docs_len, batch_size):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processing document chunk {i} of {self.docs_len}...\")\n",
    "            batch = self.docs[i : min(i + batch_size, self.docs_len)]\n",
    "            texts = [item[\"text\"] for item in batch]\n",
    "            docs_embs_batch = co.embed(\n",
    "                texts=texts, model=\"embed-english-v3.0\", input_type=\"search_document\"\n",
    "            ).embeddings\n",
    "            self.docs_embs.extend(docs_embs_batch)\n",
    "        \n",
    "        \n",
    "    def index(self) -> None:\n",
    "        \"\"\"\n",
    "        Indexes the document chunks for efficient retrieval.\n",
    "        \"\"\"\n",
    "        print(\"Indexing document chunks...\")\n",
    "\n",
    "        self.idx = hnswlib.Index(space=\"ip\", dim=1024)\n",
    "        self.idx.init_index(max_elements=self.docs_len, ef_construction=512, M=64)\n",
    "        self.idx.add_items(self.docs_embs, list(range(len(self.docs_embs))))\n",
    "\n",
    "        print(f\"Indexing complete with {self.idx.get_current_count()} document chunks.\")\n",
    "\n",
    "    def retrieve(self, query: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Retrieves document chunks based on the given query.\n",
    "\n",
    "        Parameters:\n",
    "        query (str): The query to retrieve document chunks for.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries representing the retrieved document chunks, with 'title', 'text', and 'url' keys.\n",
    "        \"\"\"\n",
    "\n",
    "        # Dense retrieval\n",
    "        query_emb = co.embed(\n",
    "            texts=[query], model=\"embed-english-v3.0\", input_type=\"search_query\"\n",
    "        ).embeddings\n",
    "        \n",
    "        doc_ids = self.idx.knn_query(query_emb, k=self.retrieve_top_k)[0][0]\n",
    "\n",
    "        # Reranking\n",
    "        rank_fields = [\"title\", \"text\"] # We'll use the title and text fields for reranking\n",
    "\n",
    "        docs_to_rerank = [self.docs[doc_id] for doc_id in doc_ids]\n",
    "        rerank_results = co.rerank(\n",
    "            query=query,\n",
    "            documents=docs_to_rerank,\n",
    "            top_n=self.rerank_top_k,\n",
    "            model=\"rerank-english-v3.0\",\n",
    "            rank_fields=rank_fields\n",
    "        )\n",
    "\n",
    "        doc_ids_reranked = [doc_ids[result.index] for result in rerank_results.results]\n",
    "\n",
    "        docs_retrieved = []\n",
    "        for doc_id in doc_ids_reranked:\n",
    "            docs_retrieved.append(\n",
    "                {\n",
    "                    \"title\": self.docs[doc_id][\"title\"],\n",
    "                    \"text\": self.docs[doc_id][\"text\"],\n",
    "                    \"url\": self.docs[doc_id][\"url\"],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return docs_retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents...\n",
      "Embedding document chunks...\n",
      "Indexing document chunks...\n",
      "Indexing complete with 172 document chunks.\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Vectorstore(raw_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot(message, chat_history=[]):\n",
    "    \n",
    "    # Generate search queries, if any        \n",
    "    response = co.chat(message=message,\n",
    "                        model=\"command-r-plus\",\n",
    "                        search_queries_only=True,\n",
    "                        chat_history=chat_history)\n",
    "    \n",
    "    search_queries = []\n",
    "    for query in response.search_queries:\n",
    "        search_queries.append(query.text)\n",
    "\n",
    "    # If there are search queries, retrieve the documents\n",
    "    if search_queries:\n",
    "        print(\"Retrieving information...\", end=\"\")\n",
    "\n",
    "        # Retrieve document chunks for each query\n",
    "        documents = []\n",
    "        for query in search_queries:\n",
    "            documents.extend(vectorstore.retrieve(query))\n",
    "\n",
    "        # Use document chunks to respond\n",
    "        response = co.chat_stream(\n",
    "            message=message,\n",
    "            model=\"command-r-plus\",\n",
    "            documents=documents,\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        response = co.chat_stream(\n",
    "            message=message,\n",
    "            model=\"command-r-plus\",\n",
    "            chat_history=chat_history,\n",
    "        )\n",
    "        \n",
    "    # Print the chatbot response and citations\n",
    "    chatbot_response = \"\"\n",
    "    print(\"\\nChatbot:\")\n",
    "\n",
    "    for event in response:\n",
    "        if event.event_type == \"text-generation\":\n",
    "            print(event.text, end=\"\")\n",
    "            chatbot_response += event.text\n",
    "        if event.event_type == \"stream-end\":\n",
    "            if event.response.citations:\n",
    "                print(\"\\n\\nCITATIONS:\")\n",
    "                for citation in event.response.citations:\n",
    "                    print(citation)\n",
    "            if event.response.documents:\n",
    "                print(\"\\nCITED DOCUMENTS:\")\n",
    "                for document in event.response.documents:\n",
    "                    print(document)\n",
    "            # Update the chat history for the next turn\n",
    "            chat_history = event.response.chat_history\n",
    "\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Advanced Prompt Engineering Techniques',\n",
       "  'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.',\n",
       "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'},\n",
       " {'title': 'Crafting Effective Prompts',\n",
       "  'text': 'Incorporating Example Outputs\\n\\nLLMs respond well when they have specific examples to work from. For example, instead of asking for the salient points of the text and using bullet points “where appropriate”, give an example of what the output should look like.',\n",
       "  'url': 'https://docs.cohere.com/docs/crafting-effective-prompts'},\n",
       " {'title': 'Advanced Prompt Engineering Techniques',\n",
       "  'text': 'In addition to giving correct examples, including negative examples with a clear indication of why they are wrong can help the LLM learn to distinguish between correct and incorrect responses. Ordering the examples can also be important; if there are patterns that could be picked up on that are not relevant to the correctness of the question, the model may incorrectly pick up on those instead of the semantics of the question itself.',\n",
       "  'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.retrieve(\"Prompting by giving examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatbot:\n",
      "Of course! I'm here to help. Please go ahead with your question, and I'll do my best to assist you."
     ]
    }
   ],
   "source": [
    "chat_history = run_chatbot(\"Hello, I have a question\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving information...\n",
      "Chatbot:\n",
      "Zero-shot prompting is when a model is asked to perform a task without being given any examples of how to do it. On the other hand, few-shot prompting is a technique where a model is provided with a few examples of the task being performed before being asked to answer a specific question. These examples help steer the model toward a high-quality solution by conditioning it to the expected response type and style.\n",
      "\n",
      "CITATIONS:\n",
      "start=0 end=19 text='Zero-shot prompting' document_ids=['doc_0', 'doc_3']\n",
      "start=63 end=95 text='without being given any examples' document_ids=['doc_0', 'doc_3']\n",
      "start=132 end=150 text='few-shot prompting' document_ids=['doc_0', 'doc_3']\n",
      "start=199 end=239 text='few examples of the task being performed' document_ids=['doc_0', 'doc_3']\n",
      "start=310 end=356 text='steer the model toward a high-quality solution' document_ids=['doc_0', 'doc_3']\n",
      "start=360 end=416 text='conditioning it to the expected response type and style.' document_ids=['doc_0', 'doc_3']\n",
      "\n",
      "CITED DOCUMENTS:\n",
      "{'id': 'doc_0', 'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n",
      "{'id': 'doc_3', 'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n"
     ]
    }
   ],
   "source": [
    "chat_history = run_chatbot(\"What's the difference between zero-shot and few-shot prompting\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving information...\n",
      "Chatbot:\n",
      "Few-shot prompting can vastly improve the quality of a model's completions. By providing a few relevant and diverse examples, the model can be steered toward a high-quality solution by conditioning it to the expected response type and style.\n",
      "\n",
      "CITATIONS:\n",
      "start=23 end=75 text=\"vastly improve the quality of a model's completions.\" document_ids=['doc_2']\n",
      "start=95 end=124 text='relevant and diverse examples' document_ids=['doc_0']\n",
      "start=143 end=181 text='steered toward a high-quality solution' document_ids=['doc_0']\n",
      "start=185 end=241 text='conditioning it to the expected response type and style.' document_ids=['doc_0']\n",
      "\n",
      "CITED DOCUMENTS:\n",
      "{'id': 'doc_2', 'text': 'Advanced Prompt Engineering Techniques\\n\\nThe previous chapter discussed general rules and heuristics to follow for successfully prompting the Command family of models. Here, we will discuss specific advanced prompt engineering techniques that can in many cases vastly improve the quality of the model’s completions. These include how to give clear and unambiguous instructions, few-shot prompting, chain-of-thought (CoT) techniques, and prompt chaining.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n",
      "{'id': 'doc_0', 'text': 'Few-shot Prompting\\n\\nUnlike the zero-shot examples above, few-shot prompting is a technique that provides a model with examples of the task being performed before asking the specific question to be answered. We can steer the LLM toward a high-quality solution by providing a few relevant and diverse examples in the prompt. Good examples condition the model to the expected response type and style.', 'title': 'Advanced Prompt Engineering Techniques', 'url': 'https://docs.cohere.com/docs/advanced-prompt-engineering-techniques'}\n"
     ]
    }
   ],
   "source": [
    "chat_history = run_chatbot(\"How would the latter help?\", chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
